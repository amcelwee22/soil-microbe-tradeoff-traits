{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b515cda",
   "metadata": {},
   "source": [
    "<h1><center>Identifying general genomic feature trade-offs and their phylogenetic relationships among soil resident microorganisms</center></h1>\n",
    "\n",
    "<center>Alicia McElwee</center>\n",
    "<center>BIOCB 6840</cen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf94ddc",
   "metadata": {},
   "source": [
    "The goal of this project is to understand how general genomic traits may help inform our understanding of microbes' ecological role in biogeochemical cycling, specifically carbon cycling. I am using sequenced and annotated genomes from the RefSoil database of soil resident microorganisms to understand what general genomic features show tradeoff characteristics that may be indicative of larger ecological tradeoffs the organism has made which influence its ecological functioning. I also want to test how phylogenetically linked these tradeoff traits are, i.e. could these tradeoff traits tell us more about an organism's ecologically functioning then their phylogenetic positioning?\n",
    "This notebook specifically contains the code used to download the genome sequences and annotations from NCBI, calculate the general genomic features, create the phylogenetic tree using PhyloPhlAn 3.0, and attempt to test phylogenetic signal using BayesTraits. The tradeoff feature analysis were done in separate applications (R and Matlab) so have been uploaded separately but are dicussed briefly here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cace90",
   "metadata": {},
   "source": [
    "<h2>Downloading Genome and Genome Annotation Files</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e2e88",
   "metadata": {},
   "source": [
    "I must first download onto my lab's server the genome sequence and genome annoatation files from NCBI where the files for the 928 RefSoil organisms are stored. For some reason the people who created the RefSoil database only give nucleotide accession and species denotation in their excel file about the database, not the assembly accession number. I also realized that many of the species names had been updated since the RefSoil list was compiled in 2015. I wrote a script to use the NCBI Entrez tool to first use the nucleotide accession number to find the updated organism name which I entered into the spreadsheet which lists all the original given data about the RefSoil entries. I then used the updated organism name to search for the assembly accession number. Finally, I used the assembly accession number to download the RefSeq genome sequence and genome annotation files from NCBI. It was stored in a folder titled Plan_B as I had originally tried downloading the database using the given organism names but realized many of those were not correct and had to redownload the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting updated organism name from nucleotide accession number included in RefSoil database\n",
    "\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "os.system(\"export PATH=${HOME}/edirect:${PATH}\")\n",
    "print(os.getcwd())\n",
    "\n",
    "with open('database_info_RefSoil_organism.tsv','r') as tsvin:\n",
    "    \n",
    "    for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        \n",
    "        if line[0] == 'RefSoil ID':\n",
    "            pass\n",
    "        else:\n",
    "            nucl_accession_line = line[1]\n",
    "            nucl_access_list = nucl_accession_line.split(',')\n",
    "            nucl_accession = nucl_access_list[0]\n",
    "            \n",
    "            nucl_acces_search_cmd_list = ['/home/alicia/edirect/esearch -db nucleotide -query', nucl_accession, '| /home/alicia/edirect/efetch -format docsum | grep -h \"<Organism>\"']\n",
    "            nucl_acces_search_cmd = ' '.join(nucl_acces_search_cmd_list)\n",
    "            #print(nucl_acces_search_cmd)\n",
    "            os.system(\"export PATH=${HOME}/edirect:${PATH}\")\n",
    "            os.system(nucl_acces_search_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting RefSeq Accession numbers for RefSoil database entries from the new organism names\n",
    "\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + os.pathsep.join([\"/home/alicia/anaconda3/envs/ncbi_datasets/bin/\"])\n",
    "print (os.environ[\"PATH\"])\n",
    "\n",
    "\n",
    "with open('Plan_B/NEW_RefSoil_organism_database_info.tsv','r') as tsvin:\n",
    "    \n",
    "    for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        \n",
    "        if line[0] == 'RefSoil ID':\n",
    "            pass\n",
    "        else:\n",
    "            ID = line[0]\n",
    "            org_name_list = [\"'\", line[18],\"'\"]\n",
    "            org_name = ''.join(org_name_list)\n",
    "            cmd_list = [\"datasets summary genome taxon\", org_name, \"--annotated\"] \n",
    "            cmd = ' '.join(cmd_list)\n",
    "            \n",
    "            p = Popen(cmd, shell=True, stdout=PIPE,  close_fds=True)\n",
    "            output = p.stdout.read()\n",
    "          \n",
    "            try:\n",
    "                output_2 = json.loads(output)\n",
    "                \n",
    "                if 'reports' in output_2:\n",
    "                    print(ID)\n",
    "                    print(output_2['reports'][0]['accession'])\n",
    "                else:\n",
    "                    print(ID)\n",
    "                    print(\"*Sad NO Buzzer noises*\") # These are to let me know that the program could not find the assembly accession using the organism name\n",
    "                    \n",
    "            except:\n",
    "                print(ID)\n",
    "                print(\"*Sad NO Buzzer noises*\") # These are to let me know that the program could not find the assembly accession using the organism name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a3f5b",
   "metadata": {},
   "source": [
    "After I did this and added the assembly accession number to the excel sheet for the entries of the RefSoil database, I had to go back and manually curate the database entries to spot check that the assembly accession numbers match the organism entry in the RefSoil database and properly marking those which had been repressed so should not be included in the analysis as, I have found, their files do not download correctly and they lead to problems later on. Of the 928 entries originally included in RefSoil I had to get rid of 38, leaving me with 890 entries used in this project. Below I use the assembly accession numbers to download the RefSoil entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d355ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downloading genome sequence and gff files for each entry using their NEW assembly accession number\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + os.pathsep.join([\"/home/alicia/anaconda3/envs/ncbi_datasets/bin/\"])\n",
    "print (os.environ[\"PATH\"])\n",
    "\n",
    "\n",
    "with open('Plan_B/NEW_RefSoil_organism_database_info.tsv','r') as tsvin:\n",
    "    for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        refsoil_ID = line[0]\n",
    "        file_name_list = [\"Plan_B/New_RefSoil_Backup/\", refsoil_ID, \".zip\"]\n",
    "        file_name = ''.join(file_name_list)\n",
    "        \n",
    "        if line[0] == 'RefSoil ID':\n",
    "            pass\n",
    "        \n",
    "        elif line[20] == 'n':\n",
    "            pass\n",
    "        \n",
    "        elif line[20] == 'y':\n",
    "            accession = line[19]\n",
    "            \n",
    "            if accession == 0 or accession == None:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                print(\"Starting: \", refsoil_ID)\n",
    "                dwnld_genome_data = [\"datasets download genome accession\", accession, \"--include gff3,genome,seq-report --annotated --assembly-source 'RefSeq' --filename\", file_name, '>/dev/null 2>&1']\n",
    "                dwnld_genome_cmd = ' '.join(dwnld_genome_data)\n",
    "                os.system(dwnld_genome_cmd)\n",
    "                print(\"Finished: \", refsoil_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff2e1a",
   "metadata": {},
   "source": [
    "<h2>Calculating General Genome Feature Values</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37139f",
   "metadata": {},
   "source": [
    "Using the genome .fna file and the annotation .gff file to determine values for the general genome features of interest. I am using one block of code to do this so I can iterate through each RefSoil entry and unzip the files only once to access the genome and genome annotation files instead of having to unzip them multiple times. For this analysis I used all 890 RefSoil entries I had downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00761e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gffutils\n",
    "from Bio import SeqIO\n",
    "from pprint import pprint\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "# Setting up keywords to use for counting each general feature of interest\n",
    "meme_trans_keys = [\"transmembrane transporter\",\"symporter\",\"antiporter\"]\n",
    "transcript_keys = [\"transcriptional regulator\",\"transcriptional repressor\",\"transcription factor\",\"operon repressor\",\"regulation of transcription\",\"transcription regulation\",\"transcription initiation\",\"transcription termination\"]\n",
    "mac_keys = [\"methyl-accepting chemotaxis protein\"]\n",
    "antibiotic_keys = [\"antibiotic biosynthesis\",\"antibiotic response\", \"response to antibiotic\", \"antibiotic catabolic\",\"antibiotic\"]\n",
    "\n",
    "# Making csv file to store calculated genome feature values in\n",
    "with open('Plan_B/NEW_Genome_feature_values.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = [\"RefSoil_ID\", \"CDS_count\", \"Coding Density\", \"rRNA_count\",\"Membrane_trans_count\",\"Membrane_trans_density\",\"TranscriptionFactor_Count\",\"TranscriptionFactor_density\", \"MAC_count\",\"MAC_density\", \"AntiBioticRelated_Count\",\"AntiBioticRelated_Density\",\"G/C Content\", \"Genome_Size\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Iterating through Truncated RefSoil entries\n",
    "    with open('Plan_B/NEW_RefSoil_organism_database_info.tsv','r') as tsvin:\n",
    "\n",
    "        for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        \n",
    "            if line[0] == 'RefSoil ID':\n",
    "                pass\n",
    "        \n",
    "            elif line[20] == 'n': # Using list for truncated RefSoil or either y or n\n",
    "                pass\n",
    "            \n",
    "            elif line[20] == 'y':\n",
    "                \n",
    "                # Collecting needed info\n",
    "                refsoil_ID = line[0]\n",
    "                refsoil_ID_loc_list = ['Plan_B/', refsoil_ID]\n",
    "                refsoil_ID_loc = ''.join(refsoil_ID_loc_list)\n",
    "                accession = line[19]\n",
    "                \n",
    "                # Uncompressing RefSoil files for the entry\n",
    "                zip_file_name_list = [\"Plan_B/New_RefSoil/\",refsoil_ID, \".zip\"]\n",
    "                zip_file_name = ''.join(zip_file_name_list)\n",
    "            \n",
    "                unzip_cmd_list = ['unzip -d', refsoil_ID_loc, zip_file_name]\n",
    "                unzip_cmd = ' '.join(unzip_cmd_list)\n",
    "                os.system(unzip_cmd)\n",
    "                \n",
    "                # Finding genome sequence file path\n",
    "                genome_file_loc_list = [refsoil_ID_loc, \"/ncbi_dataset/data/\", accession, \"/G*\"]\n",
    "                genome_file_loc = ''.join(genome_file_loc_list)\n",
    "                genome_file_loc = ''.join(glob.glob(genome_file_loc))\n",
    "                \n",
    "                # Finding gff file path\n",
    "                gff_file_loc_list = [refsoil_ID_loc, \"/ncbi_dataset/data/\", accession, \"/genomic.gff\"]\n",
    "                gff_file_loc = ''.join(gff_file_loc_list)\n",
    "\n",
    "                # Creating gff database with gffutils to search through for features of interest\n",
    "                db = gffutils.create_db(gff_file_loc, \"Plan_B/gff.db\", merge_strategy = \"create_unique\", keep_order = True)\n",
    "                \n",
    "                # Setting Count values back to 0 for next entry\n",
    "                rRNA_count = 0\n",
    "                CDS_count = 0\n",
    "                memeTrans_count = 0\n",
    "                transcript_count = 0\n",
    "                mac_count = 0\n",
    "                antiBiotic_count = 0\n",
    "\n",
    "                # Iterating through all genome features\n",
    "                for feature in db.all_features():\n",
    "\n",
    "                    # Counting number of copies of rRNA based on number of 16S genes\n",
    "                    if feature.attributes['gbkey']==['rRNA']:\n",
    "                        if '16S' in feature.attributes['product'][0]:\n",
    "                            rRNA_count +=1\n",
    "\n",
    "                    if feature.attributes['gbkey'] == ['CDS']:\n",
    "\n",
    "                        # Counting Number of coding sequences\n",
    "                        CDS_count +=1\n",
    "\n",
    "                        try:\n",
    "                            go_product = ''.join(feature.attributes['product'])\n",
    "\n",
    "                            # Counting Methly-accepting chemotaxis genes\n",
    "                            if any(x in go_product for x in mac_keys):\n",
    "                                mac_count +=1 \n",
    "\n",
    "                            # Counting Some of the antiobiotic related genes\n",
    "                            if any(x in go_product for x in antibiotic_keys):\n",
    "                                antiBiotic_count +=1 \n",
    "\n",
    "                            else:\n",
    "\n",
    "                                try: \n",
    "                                    go_function = ''.join(feature.attributes['go_function'])\n",
    "                                    go_process = ''.join(feature.attributes['go_process'])\n",
    "\n",
    "                                    # Counting Membrane Transporter Genes\n",
    "                                    if any(x in go_function for x in meme_trans_keys) or any(x in go_process for x in meme_trans_keys) or any(x in go_product for x in meme_trans_keys):\n",
    "                                        memeTrans_count +=1      \n",
    "\n",
    "                                    # Counting Transcription Factor Genes\n",
    "                                    if any(x in go_function for x in transcript_keys) or any(x in go_process for x in transcript_keys) or any(x in go_product for x in transcript_keys):\n",
    "                                        transcript_count +=1\n",
    "\n",
    "                                    # Counting Rest of antibiotic related genes\n",
    "                                    if any(x in go_function for x in antibiotic_keys) or any(x in go_process for x in antibiotic_keys):\n",
    "                                        antiBiotic_count +=1\n",
    "\n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "\n",
    "                # Finding Genome Length\n",
    "                print(genome_file_loc)\n",
    "                f = open(genome_file_loc, \"r\")\n",
    "                seq_len = 0\n",
    "                G_C = 0\n",
    "\n",
    "                for line in f:\n",
    "                    if line.startswith('>'):\n",
    "                        pass\n",
    "                    else:\n",
    "                        line_bp_count = len(line)\n",
    "                        seq_len = seq_len + line_bp_count\n",
    "                        for bp in line:\n",
    "                            if bp == 'g' or bp == 'G' or bp == 'c' or bp == 'C':\n",
    "                                G_C += 1\n",
    "                    \n",
    "                # Calculating other values of interest\n",
    "                G_C_content = G_C/seq_len\n",
    "                coding_density = CDS_count/seq_len\n",
    "                memeTrans_density = memeTrans_count/CDS_count\n",
    "                transcript_density = transcript_count/CDS_count\n",
    "                mac_density = mac_count/CDS_count\n",
    "                antiBiotic_density = antiBiotic_count/CDS_count\n",
    "                \n",
    "\n",
    "                new_row = [refsoil_ID, CDS_count, coding_density, rRNA_count, memeTrans_count, memeTrans_density, transcript_count, transcript_density, mac_count, mac_density, antiBiotic_count, antiBiotic_density, G_C_content, seq_len]\n",
    "                writer.writerow(new_row)\n",
    "                \n",
    "                # Deleting current gff.db file\n",
    "                os.system(\"rm Plan_B/gff.db\")\n",
    "                \n",
    "                # Deleting uncompressed file\n",
    "                cmd_rm_list = [\"rm -r\", refsoil_ID_loc]\n",
    "                cmd_rm = ' '.join(cmd_rm_list)\n",
    "                os.system(cmd_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c1108",
   "metadata": {},
   "source": [
    "The code above produced the New_Genome_feature_values.csv output with all 13 general genomic feature values calculated for all 890 RefSoil entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36638901",
   "metadata": {},
   "source": [
    "<h2>Analysis of Genome Features for Tradeoff Relationships</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31b44f",
   "metadata": {},
   "source": [
    "<h3>Analysis with ParTI</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c108c",
   "metadata": {},
   "source": [
    "For the ParTI analysis, I wrote the new_genome_feature_analysis.m script located in the Tradeoff_Feature_Analysis/ParTI folder based on the example scripts that came with the program. The inputs to this program included: values_only_LOG_NewGenomeNewFeaturevalues.csv which contains the log transformed feature values with the header column and the first row identifying which RefSoil entry the data is from removed (as is required by ParTI) and feature_names.list which is a list of the genome feature names to try to input the into the continuous feature parameters to do enrichment analysis of the features in the different identified archetypes. The outputs produced by this program are in Tradeoff_Feature_Analysis/ParTI/Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcae7a2",
   "metadata": {},
   "source": [
    "<h3>Backup R PCA Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5610e0",
   "metadata": {},
   "source": [
    "Since I was unable to get the feature enrichment analysis to work in ParTI, I did a simple PCA analysis in R. The R markdown file for this (Backup_R_PCA_Analysis.Rmd) is in Tradeoff_Feature_Analysis/R and the input file is LOGTranform_NewGenomeNewFeaturevalues.xlsx. Note that this file contains the exact same log transformed feature values as was used in the ParTI analysis but retains the entry identification column and the row with the column headers. The 2D PCA loading plot is found in Tradeoff_Feature_Analysis/R/2D_Feature_PCAloadingPlot.png. I could not save the 3D loading plot as it requires the rgl package to view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf30395",
   "metadata": {},
   "source": [
    "<h2>Phylogenetic Tree Creation with PhyloPhlAn 3.0 </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97391f7c",
   "metadata": {},
   "source": [
    "I used PhyloPhlAn 3.0 (Asnicar et al. 2020. Nature Communications, 11(1), Article 1) to construct a phylogenetic tree from the genome sequences of all 890 entries using 400 marker genes. I ran this program through the command line on my labs server in an anaconda environment but used this notebook to the preprocessing steps. I followed the instructions for setting parameters and running the program's' supertree method found at https://github.com/biobakery/phylophlan/wiki. I realized quickly that creating a multi-locus tree would too long for the time span of this project, so I decided to truncate my dataset from 890 to ~200 entries using a weighted random yes or no generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b37934",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating Weighted Random y or n for 887 entries to select around ~200 entries for further analysis\n",
    "\n",
    "import csv\n",
    "import random\n",
    "\n",
    "with open('database_info_RefSoil_organism.tsv','r') as tsvin:\n",
    "    \n",
    "    for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        \n",
    "        if line[0] == 'RefSoil ID':\n",
    "            pass\n",
    "        elif line[18] == 'n': # If entry was already not part of the 887, then put n again for it\n",
    "            print('n') \n",
    "        elif line[18] == 'y': # If entry was a part of the 887, then take weighted random choice of n or y\n",
    "            rand_value = ''.join(random.choices(['n','y'], weights = (78,22)))\n",
    "            print(rand_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e645304",
   "metadata": {},
   "source": [
    "I then copied these weighted random y's and n's into the Excel file about the database entries and used it to denote which entries to delete from my current working to clean it up directory before proceeding with phylogenetic tree creation. There were 203 entries which were selected to use for downstream phylogenetic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332abf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Random Weighted y and n values to truncate RefSoil entries for this project (deleting them from working directory)\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "with open('Plan_B/NEW_RefSoil_organism_database_info.tsv','r') as tsvin:\n",
    "    \n",
    "    for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        \n",
    "        if line[0] == 'RefSoil ID':\n",
    "            pass\n",
    "        \n",
    "        elif line[21] == 'y':\n",
    "            pass\n",
    "        \n",
    "        elif line[21] == 'n':\n",
    "            ID = line[0]\n",
    "            zip_name_list = ['Plan_B/New_RefSoil/', ID, '.zip']\n",
    "            zip_name = ''.join(zip_name_list)\n",
    "            \n",
    "            rm_cmd_list = ['rm', zip_name]\n",
    "            rm_cmd = ' '.join(rm_cmd_list)\n",
    "            os.system(rm_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04e5f2",
   "metadata": {},
   "source": [
    "I next had to copy all the genome sequence files from the 203 entries for phylogenetic analysis into a single folder and convert them to a zip format to save space on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collecting genome files of Truncated RefSoil into a single folder for PhyloPhlAn 3.0\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "with open('Plan_B/NEW_RefSoil_organism_database_info.tsv','r') as tsvin:\n",
    "\n",
    "    for line in csv.reader(tsvin, delimiter='\\t'):\n",
    "        \n",
    "        if line[0] == 'RefSoil ID':\n",
    "            pass\n",
    "        \n",
    "        elif line[21] == 'n': # Using list for truncated RefSoil or either y or n\n",
    "            pass\n",
    "            \n",
    "        elif line[21] == 'y':\n",
    "            \n",
    "            # Getting needed info\n",
    "            refsoil_ID = line[0]\n",
    "            refsoil_file_list =['Plan_B/New_RefSoil/', refsoil_ID]\n",
    "            refsoil_file = ''.join(refsoil_file_list)\n",
    "            \n",
    "            accession = line[19]\n",
    "            print(refsoil_ID)\n",
    "            zip_file_name_list = ['Plan_B/New_RefSoil/', refsoil_ID, \".zip\"]\n",
    "            zip_file_name = ''.join(zip_file_name_list)\n",
    "            \n",
    "            \n",
    "            # Unzipping File\n",
    "            unzip_cmd_list = ['unzip -d', refsoil_file, zip_file_name]\n",
    "            unzip_cmd = ' '.join(unzip_cmd_list)\n",
    "            os.system(unzip_cmd)\n",
    "            \n",
    "            # Compressing genome sequence file\n",
    "            genome_file_loc_list = ['Plan_B/New_RefSoil/', refsoil_ID, \"/ncbi_dataset/data/\", accession, \"/G*\"]\n",
    "            genome_file_loc = ''.join(genome_file_loc_list)\n",
    "            cmpress_cmd_list = [\"gzip\", genome_file_loc]\n",
    "            cmpress_cmd = ' '.join(cmpress_cmd_list)\n",
    "            os.system(cmpress_cmd)\n",
    "            \n",
    "            # Renaming genome sequence file\n",
    "            New_compressed_loc_list = ['Plan_B/New_RefSoil/', refsoil_ID, \"/ncbi_dataset/data/\", accession,\"/\", refsoil_ID, '.fna.gz']\n",
    "            New_compressed_loc = ''.join(New_compressed_loc_list)\n",
    "            Old_compressed_loc_list = ['Plan_B/New_RefSoil/', refsoil_ID, \"/ncbi_dataset/data/\", accession, \"/G*\"]\n",
    "            Old_compressed_loc = ''.join(Old_compressed_loc_list)\n",
    "            cmd_rename_list = [\"mv\", Old_compressed_loc, New_compressed_loc]\n",
    "            cmd_rename = ' '.join(cmd_rename_list)\n",
    "            os.system(cmd_rename)\n",
    "            \n",
    "            # Moving compressed sequence file to genome_files folder\n",
    "            destination_name_list = [\"/home/alicia/class/Comp_gene/Plan_B/New_genome_files/\", refsoil_ID, '.fna.gz']\n",
    "            destination_name = ''.join(destination_name_list)\n",
    "            cmd_mv_list = [\"mv\", New_compressed_loc, destination_name]\n",
    "            cmd_mv = ' '.join(cmd_mv_list)\n",
    "            print(cmd_mv)\n",
    "            os.system(cmd_mv)\n",
    "            \n",
    "            # Deleting uncompressed file\n",
    "            cmd_rm_list = [\"rm -r\", refsoil_file]\n",
    "            cmd_rm = ' '.join(cmd_rm_list)\n",
    "            os.system(cmd_rm)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afaddaf",
   "metadata": {},
   "source": [
    "I was now ready to run PhyloPhlAn 3.0 on my server, which I did using the command line code below. New_genome_files is the name of the directory where the genome sequence files for the 203 RefSoil entries were copied to. Note that supertree_aa.cfg is a default configuration file made by PhyloPhlAn where I only had to update the area asking where ASTRAL was located. phylophlan.tsv is a list of which substitution model to use for each marker gene when analyzing the multi-sequence alignments to create the gene trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a852dd9",
   "metadata": {},
   "source": [
    "<code>phylophlan -i New_genome_files -d phylophlan --diversity high -f supertree_aa.cfg --force_nucleotides --maas phylophlan.tsv  --accurate --nproc 50</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff2da5",
   "metadata": {},
   "source": [
    "This produced the New_genome_files.tre output containing the phylogenetic tree in Newick format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d385d6",
   "metadata": {},
   "source": [
    "<h2>Testing Phylogenetic Signal of Features</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556854ee",
   "metadata": {},
   "source": [
    "This program was run on the command line of both my lab's server and my personal computer. In both cases, the inputs were phylophlan_accurate_with_NewRefSoil_genome_features.nex (the phylogenetic tree which I had to convert from Newick to Nexus format using the Newick2Nexus tool (https://github.com/josephwb/Newick2Nexus) since BayesTraits only uses Nexus trees) and LOGTranform_NEW_GenomeNEWFeaturevalues.tsv (the log transformed feature values with the column headers removed as is required for BayesTraits). These input files can be found in the BayesTraits_Input folder. The command use to run the program was:   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7b7be",
   "metadata": {},
   "source": [
    "<code>BayesTraitsV4 phylophlan_accurate_with_NewRefSoil_genome_features.nex LOGTranform_NEWGenomeNEWFeaturevalues.tsv</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e80067",
   "metadata": {},
   "source": [
    "But when I ran this program on both the lab server and my personal computer, it would not then prompt me to select an analysis method. It would just produce a bunch of numbers and then exit the program. I tried to solve this but did ran out of time with this project o figure out why that was happening."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
